# BERT Crystal Collection

**Architecture:** Encoder-Only Transformer (Bidirectional)  
**Shape:** The "Stability Pillar"  
**Concept:** Structural Understanding

BERT (Bidirectional Encoder Representations from Transformers) reads text from both directions simultaneously. Unlike GPT-2's twisting production line, BERT forms a rigid, symmetrical column. It is built to "hold" information in place for analysis rather than generating new tokens.

## Visualizations

### 1. Structural Crystals
*   **`structure_layers.ply`**: The "base" architecture. notice the uniform width and lack of torsion compared to GPT-2. This represents the stable, fixed-size context window processing (768 hidden size) across all 12 layers.

### 2. Cognitive Heatmaps (Activations)
*   **`activation_quick_brown_fox.ply`**: *Input: "The quick brown fox"*  
    Because BERT sees the whole sentence at once, the activation pattern is not a linear path but a distributed "web" of activity. You can see red hotspots scattered evenly throughout the pillar, showing parallel processing of the entire context.
