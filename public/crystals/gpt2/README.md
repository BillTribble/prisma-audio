# GPT-2 Crystal Collection

**Architecture:** Decoder-Only Transformer  
**Shape:** The "Helix Tower"  
**Concept:** Generative Flow (Chaos to Order)

GPT-2 is a decoder model designed to predict the next token in a sequence. Its weight structure naturally forms a twisting tower that narrows slightly as information is refined from raw embeddings (bottom) to final predictions (top).

## Visualizations

### 1. Structural Crystals
*   **`structure_solid.ply`**: The definitive "Solid State" visualization. High-density sampling with diagonal cross-lacing to simulate a solid crystal surface. Shows the full 12-layer depth.
*   **`structure_layers.ply`**: Colored by layer depth (Spectrum). Red (Input) -> Violet (Output). Best for seeing the vertical flow of information.
*   **`structure_lattice.ply`**: A lighter wireframe version, highlighting the connectivity graph between neurons.

### 2. Cognitive Heatmaps (Activations)
These crystals show the model "thinking" in real-time. The red/white hotspots represent neurons that fired strongly for a specific input.

*   **`activation_future.ply`**: *Input: "The future is vast and infinite"*  
    Shows a broad, hopeful activation pattern, using widespread general knowledge headers.
*   **`activation_quantum.ply`**: *Input: "Quantum physics is confusing"*  
    Shows a distinct, tighter vertical column of activation, indicating the model is accessing specific scientific/technical sub-circuits.
