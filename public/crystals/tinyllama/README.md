# TinyLlama Crystal Collection

**Architecture:** Modern Llama Decoder (RoPE, SwiGLU)  
**Shape:** The "Dense Spiral"  
**Concept:** Complex Reasoning (1.1 Billion Parameters)

TinyLlama represents the modern era of LLMs (post-2023). While it shares the "Tower" shape with GPT-2, its internal lattice is far denser and more complex due to:
*   **Rotary Positional Embeddings (RoPE)**: Adds a spiral/rotational quality to the math.
*   **SwiGLU Activations**: Sharper, more precise gating of information.
*   **Depth**: 22 Layers (vs GPT-2's 12).

## Visualizations

### 1. Cognitive Heatmaps (Activations)
*   **`activation_consciousness.ply`**: *Input: "Consciousness is a strange loop"*  
    A massive 1.1 billion parameter crystal structure. The sheer density of connections here shows why modern LLMs are so much more capable than their ancestors. The "thought" is a complex, multi-pathway surge of energy running through the deep spine of the model.
